{"config":{"indexing":"full","lang":["fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Accueil"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"models/vovnet/","text":"VoVNet : An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection Abstract As DenseNet conserves intermediate features with diverse receptive fields by aggregating them with dense connection, it shows good performance on the object detection task. Although feature reuse enables DenseNet to produce strong features with a small number of model parameters and FLOPs, the detector with DenseNet backbone shows rather slow speed and low energy efficiency. We find the linearly increasing input channel by dense connection leads to heavy memory access cost, which causes computation overhead and more energy consumption. To solve the inefficiency of DenseNet, we propose an energy and computation efficient architecture called VoVNet comprised of One-Shot Aggregation (OSA). The OSA not only adopts the strength of DenseNet that represents diversified features with multi receptive fields but also overcomes the inefficiency of dense connection by aggregating all features only once in the last feature maps. To validate the effectiveness of VoVNet as a backbone network, we design both lightweight and large-scale VoVNet and apply them to one-stage and two-stage object detectors. Our VoVNet based detectors outperform DenseNet based ones with 2x faster speed and the energy consumptions are reduced by 1.6x - 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet backbone with faster speed and better energy efficiency. In particular, the small object detection performance has been significantly improved over DenseNet and ResNet. ArXiv link OSAModule Bases: Layer summary Inheritance tf.keras.layers.Layer: Source code in core_vision/models/vovnet.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @tf . keras . utils . register_keras_serializable () class OSAModule ( Layer ): \"\"\"_summary_ Inheritance: tf.keras.layers.Layer: \"\"\" def __init__ ( self , filters_conv3x3 : int , filters_conv1x1 : int , * args , ** kwargs , ) -> None : \"\"\"_summary_ Args: filters_conv3x3 (int): _description_ filters_conv1x1 (int): _description_ Info: Input[B, H, W, C_1] --> Output[B, H, W, C_2] where C_2 = `filters_conv1x1` \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters_conv3x3 = filters_conv3x3 self . filters_conv1x1 = filters_conv1x1 self . concat = Concatenate ( axis =- 1 ) def build ( self , input_shape ) -> None : self . conv1 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv2 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv3 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv4 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv5 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv6 = ConvBNReLU ( filters = self . filters_conv1x1 , kernel_size = 1 , ) def call ( self , inputs : tf . Tensor , training = None ) -> tf . Tensor : \"\"\"_summary_ Args: inputs (tf.Tensor): _description_ training (_type_, optional): _description_. Defaults to None. Returns: tf.Tensor: _description_ \"\"\" fmap1 = self . conv1 ( inputs ) fmap2 = self . conv2 ( inputs ) fmap3 = self . conv3 ( inputs ) fmap4 = self . conv4 ( inputs ) fmap5 = self . conv5 ( inputs ) fmap = self . concat ([ fmap1 , fmap2 , fmap3 , fmap4 , fmap5 ]) return self . conv6 ( fmap ) def get_config ( self ) -> Dict [ str , Any ]: \"\"\"_summary_ Returns: Dict[str, Any]: _description_ \"\"\" config = super () . get_config () config . update ( { \"filters_conv3x3\" : self . filters_conv3x3 , \"filters_conv1x1\" : self . filters_conv1x1 , }, ) return config @classmethod def from_config ( cls , config ): \"\"\"_summary_ Args: config (_type_): _description_ Returns: _type_: _description_ \"\"\" return cls ( ** config ) __init__ ( filters_conv3x3 , filters_conv1x1 , * args , ** kwargs ) summary Parameters: Name Type Description Default filters_conv3x3 int description required filters_conv1x1 int description required Info Input[B, H, W, C_1] --> Output[B, H, W, C_2] where C_2 = filters_conv1x1 Source code in core_vision/models/vovnet.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , filters_conv3x3 : int , filters_conv1x1 : int , * args , ** kwargs , ) -> None : \"\"\"_summary_ Args: filters_conv3x3 (int): _description_ filters_conv1x1 (int): _description_ Info: Input[B, H, W, C_1] --> Output[B, H, W, C_2] where C_2 = `filters_conv1x1` \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters_conv3x3 = filters_conv3x3 self . filters_conv1x1 = filters_conv1x1 self . concat = Concatenate ( axis =- 1 ) call ( inputs , training = None ) summary Parameters: Name Type Description Default inputs tf . Tensor description required training _type_ description . Defaults to None. None Returns: Type Description tf . Tensor tf.Tensor: description Source code in core_vision/models/vovnet.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def call ( self , inputs : tf . Tensor , training = None ) -> tf . Tensor : \"\"\"_summary_ Args: inputs (tf.Tensor): _description_ training (_type_, optional): _description_. Defaults to None. Returns: tf.Tensor: _description_ \"\"\" fmap1 = self . conv1 ( inputs ) fmap2 = self . conv2 ( inputs ) fmap3 = self . conv3 ( inputs ) fmap4 = self . conv4 ( inputs ) fmap5 = self . conv5 ( inputs ) fmap = self . concat ([ fmap1 , fmap2 , fmap3 , fmap4 , fmap5 ]) return self . conv6 ( fmap ) from_config ( config ) classmethod summary Parameters: Name Type Description Default config _type_ description required Returns: Name Type Description _type_ description Source code in core_vision/models/vovnet.py 103 104 105 106 107 108 109 110 111 112 113 @classmethod def from_config ( cls , config ): \"\"\"_summary_ Args: config (_type_): _description_ Returns: _type_: _description_ \"\"\" return cls ( ** config ) get_config () summary Returns: Type Description Dict [ str , Any ] Dict[str, Any]: description Source code in core_vision/models/vovnet.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def get_config ( self ) -> Dict [ str , Any ]: \"\"\"_summary_ Returns: Dict[str, Any]: _description_ \"\"\" config = super () . get_config () config . update ( { \"filters_conv3x3\" : self . filters_conv3x3 , \"filters_conv1x1\" : self . filters_conv1x1 , }, ) return config VoVNet Bases: TFModel Source code in core_vision/models/vovnet.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class VoVNet ( TFModel ): def __init__ ( self , img_shape : List [ int ], filters_conv3x3 : List [ int ], filters_conv1x1 : List [ int ], block_repetitions : List [ int ], name : str , ) -> None : \"\"\"_summary_ Args: img_shape (List[int]): _description_ filters_conv3x3 (List[int]): _description_ filters_conv1x1 (List[int]): _description_ block_repetitions (List[int]): _description_ name (str): _description_ \"\"\" self . img_shape = img_shape self . filters_conv3x3 = filters_conv3x3 self . filters_conv1x1 = filters_conv1x1 self . block_repetitions = block_repetitions self . name = name self . endpoint_layers = [ \"maxpool_block1_out\" , \"maxpool_block2_out\" , \"maxpool_block3_out\" , \"maxpool_block4_out\" , ] def get_classification_backbone ( self ) -> Model : \"\"\"_summary_ Returns: Model: _description_ \"\"\" block1 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 0 ], filters_conv1x1 = self . filters_conv1x1 [ 0 ], name = f \"block_1_ { idx } \" , ) for idx in range ( self . block_repetitions [ 0 ]) ] block2 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 1 ], filters_conv1x1 = self . filters_conv1x1 [ 1 ], name = f \"block_2_ { idx } \" , ) for idx in range ( self . block_repetitions [ 1 ]) ] block3 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 2 ], filters_conv1x1 = self . filters_conv1x1 [ 2 ], name = f \"block_3_ { idx } \" , ) for idx in range ( self . block_repetitions [ 2 ]) ] block4 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 3 ], filters_conv1x1 = self . filters_conv1x1 [ 3 ], name = f \"block_4_ { idx } \" , ) for idx in range ( self . block_repetitions [ 3 ]) ] return Sequential ( [ Input ( self . img_shape ), ConvBNReLU ( filters = 64 , kernel_size = 3 , strides = 2 , name = \"stem_stage_1\" , ), ConvBNReLU ( filters = 64 , kernel_size = 3 , name = \"stem_stage_2\" , ), ConvBNReLU ( filters = 128 , kernel_size = 3 , name = \"stem_stage_3\" , ), * block1 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block1_out\" ), * block2 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block2_out\" ), * block3 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block3_out\" ), * block4 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block4_out\" ), ], name = self . name , ) def get_segmentation_backbone ( self ) -> Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Returns: A `tf.keras` model. \"\"\" backbone = self . get_classification_backbone () os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in self . endpoint_layers ] return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = self . name , ) __init__ ( img_shape , filters_conv3x3 , filters_conv1x1 , block_repetitions , name ) summary Parameters: Name Type Description Default img_shape List [ int ] description required filters_conv3x3 List [ int ] description required filters_conv1x1 List [ int ] description required block_repetitions List [ int ] description required name str description required Source code in core_vision/models/vovnet.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self , img_shape : List [ int ], filters_conv3x3 : List [ int ], filters_conv1x1 : List [ int ], block_repetitions : List [ int ], name : str , ) -> None : \"\"\"_summary_ Args: img_shape (List[int]): _description_ filters_conv3x3 (List[int]): _description_ filters_conv1x1 (List[int]): _description_ block_repetitions (List[int]): _description_ name (str): _description_ \"\"\" self . img_shape = img_shape self . filters_conv3x3 = filters_conv3x3 self . filters_conv1x1 = filters_conv1x1 self . block_repetitions = block_repetitions self . name = name self . endpoint_layers = [ \"maxpool_block1_out\" , \"maxpool_block2_out\" , \"maxpool_block3_out\" , \"maxpool_block4_out\" , ] get_classification_backbone () summary Returns: Name Type Description Model Model description Source code in core_vision/models/vovnet.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def get_classification_backbone ( self ) -> Model : \"\"\"_summary_ Returns: Model: _description_ \"\"\" block1 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 0 ], filters_conv1x1 = self . filters_conv1x1 [ 0 ], name = f \"block_1_ { idx } \" , ) for idx in range ( self . block_repetitions [ 0 ]) ] block2 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 1 ], filters_conv1x1 = self . filters_conv1x1 [ 1 ], name = f \"block_2_ { idx } \" , ) for idx in range ( self . block_repetitions [ 1 ]) ] block3 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 2 ], filters_conv1x1 = self . filters_conv1x1 [ 2 ], name = f \"block_3_ { idx } \" , ) for idx in range ( self . block_repetitions [ 2 ]) ] block4 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 3 ], filters_conv1x1 = self . filters_conv1x1 [ 3 ], name = f \"block_4_ { idx } \" , ) for idx in range ( self . block_repetitions [ 3 ]) ] return Sequential ( [ Input ( self . img_shape ), ConvBNReLU ( filters = 64 , kernel_size = 3 , strides = 2 , name = \"stem_stage_1\" , ), ConvBNReLU ( filters = 64 , kernel_size = 3 , name = \"stem_stage_2\" , ), ConvBNReLU ( filters = 128 , kernel_size = 3 , name = \"stem_stage_3\" , ), * block1 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block1_out\" ), * block2 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block2_out\" ), * block3 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block3_out\" ), * block4 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block4_out\" ), ], name = self . name , ) get_segmentation_backbone () Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Returns: Type Description Model A tf.keras model. Source code in core_vision/models/vovnet.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def get_segmentation_backbone ( self ) -> Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Returns: A `tf.keras` model. \"\"\" backbone = self . get_classification_backbone () os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in self . endpoint_layers ] return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = self . name , )","title":"VoVnet"},{"location":"models/vovnet/#vovnet-an-energy-and-gpu-computation-efficient-backbone-network-for-real-time-object-detection","text":"Abstract As DenseNet conserves intermediate features with diverse receptive fields by aggregating them with dense connection, it shows good performance on the object detection task. Although feature reuse enables DenseNet to produce strong features with a small number of model parameters and FLOPs, the detector with DenseNet backbone shows rather slow speed and low energy efficiency. We find the linearly increasing input channel by dense connection leads to heavy memory access cost, which causes computation overhead and more energy consumption. To solve the inefficiency of DenseNet, we propose an energy and computation efficient architecture called VoVNet comprised of One-Shot Aggregation (OSA). The OSA not only adopts the strength of DenseNet that represents diversified features with multi receptive fields but also overcomes the inefficiency of dense connection by aggregating all features only once in the last feature maps. To validate the effectiveness of VoVNet as a backbone network, we design both lightweight and large-scale VoVNet and apply them to one-stage and two-stage object detectors. Our VoVNet based detectors outperform DenseNet based ones with 2x faster speed and the energy consumptions are reduced by 1.6x - 4.1x. In addition to DenseNet, VoVNet also outperforms widely used ResNet backbone with faster speed and better energy efficiency. In particular, the small object detection performance has been significantly improved over DenseNet and ResNet. ArXiv link","title":"VoVNet : An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection"},{"location":"models/vovnet/#core_vision.models.vovnet.OSAModule","text":"Bases: Layer summary Inheritance tf.keras.layers.Layer: Source code in core_vision/models/vovnet.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @tf . keras . utils . register_keras_serializable () class OSAModule ( Layer ): \"\"\"_summary_ Inheritance: tf.keras.layers.Layer: \"\"\" def __init__ ( self , filters_conv3x3 : int , filters_conv1x1 : int , * args , ** kwargs , ) -> None : \"\"\"_summary_ Args: filters_conv3x3 (int): _description_ filters_conv1x1 (int): _description_ Info: Input[B, H, W, C_1] --> Output[B, H, W, C_2] where C_2 = `filters_conv1x1` \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters_conv3x3 = filters_conv3x3 self . filters_conv1x1 = filters_conv1x1 self . concat = Concatenate ( axis =- 1 ) def build ( self , input_shape ) -> None : self . conv1 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv2 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv3 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv4 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv5 = ConvBNReLU ( filters = self . filters_conv3x3 , kernel_size = 3 , ) self . conv6 = ConvBNReLU ( filters = self . filters_conv1x1 , kernel_size = 1 , ) def call ( self , inputs : tf . Tensor , training = None ) -> tf . Tensor : \"\"\"_summary_ Args: inputs (tf.Tensor): _description_ training (_type_, optional): _description_. Defaults to None. Returns: tf.Tensor: _description_ \"\"\" fmap1 = self . conv1 ( inputs ) fmap2 = self . conv2 ( inputs ) fmap3 = self . conv3 ( inputs ) fmap4 = self . conv4 ( inputs ) fmap5 = self . conv5 ( inputs ) fmap = self . concat ([ fmap1 , fmap2 , fmap3 , fmap4 , fmap5 ]) return self . conv6 ( fmap ) def get_config ( self ) -> Dict [ str , Any ]: \"\"\"_summary_ Returns: Dict[str, Any]: _description_ \"\"\" config = super () . get_config () config . update ( { \"filters_conv3x3\" : self . filters_conv3x3 , \"filters_conv1x1\" : self . filters_conv1x1 , }, ) return config @classmethod def from_config ( cls , config ): \"\"\"_summary_ Args: config (_type_): _description_ Returns: _type_: _description_ \"\"\" return cls ( ** config )","title":"OSAModule"},{"location":"models/vovnet/#core_vision.models.vovnet.OSAModule.__init__","text":"summary Parameters: Name Type Description Default filters_conv3x3 int description required filters_conv1x1 int description required Info Input[B, H, W, C_1] --> Output[B, H, W, C_2] where C_2 = filters_conv1x1 Source code in core_vision/models/vovnet.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __init__ ( self , filters_conv3x3 : int , filters_conv1x1 : int , * args , ** kwargs , ) -> None : \"\"\"_summary_ Args: filters_conv3x3 (int): _description_ filters_conv1x1 (int): _description_ Info: Input[B, H, W, C_1] --> Output[B, H, W, C_2] where C_2 = `filters_conv1x1` \"\"\" super () . __init__ ( * args , ** kwargs ) self . filters_conv3x3 = filters_conv3x3 self . filters_conv1x1 = filters_conv1x1 self . concat = Concatenate ( axis =- 1 )","title":"__init__()"},{"location":"models/vovnet/#core_vision.models.vovnet.OSAModule.call","text":"summary Parameters: Name Type Description Default inputs tf . Tensor description required training _type_ description . Defaults to None. None Returns: Type Description tf . Tensor tf.Tensor: description Source code in core_vision/models/vovnet.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def call ( self , inputs : tf . Tensor , training = None ) -> tf . Tensor : \"\"\"_summary_ Args: inputs (tf.Tensor): _description_ training (_type_, optional): _description_. Defaults to None. Returns: tf.Tensor: _description_ \"\"\" fmap1 = self . conv1 ( inputs ) fmap2 = self . conv2 ( inputs ) fmap3 = self . conv3 ( inputs ) fmap4 = self . conv4 ( inputs ) fmap5 = self . conv5 ( inputs ) fmap = self . concat ([ fmap1 , fmap2 , fmap3 , fmap4 , fmap5 ]) return self . conv6 ( fmap )","title":"call()"},{"location":"models/vovnet/#core_vision.models.vovnet.OSAModule.from_config","text":"summary Parameters: Name Type Description Default config _type_ description required Returns: Name Type Description _type_ description Source code in core_vision/models/vovnet.py 103 104 105 106 107 108 109 110 111 112 113 @classmethod def from_config ( cls , config ): \"\"\"_summary_ Args: config (_type_): _description_ Returns: _type_: _description_ \"\"\" return cls ( ** config )","title":"from_config()"},{"location":"models/vovnet/#core_vision.models.vovnet.OSAModule.get_config","text":"summary Returns: Type Description Dict [ str , Any ] Dict[str, Any]: description Source code in core_vision/models/vovnet.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def get_config ( self ) -> Dict [ str , Any ]: \"\"\"_summary_ Returns: Dict[str, Any]: _description_ \"\"\" config = super () . get_config () config . update ( { \"filters_conv3x3\" : self . filters_conv3x3 , \"filters_conv1x1\" : self . filters_conv1x1 , }, ) return config","title":"get_config()"},{"location":"models/vovnet/#core_vision.models.vovnet.VoVNet","text":"Bases: TFModel Source code in core_vision/models/vovnet.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class VoVNet ( TFModel ): def __init__ ( self , img_shape : List [ int ], filters_conv3x3 : List [ int ], filters_conv1x1 : List [ int ], block_repetitions : List [ int ], name : str , ) -> None : \"\"\"_summary_ Args: img_shape (List[int]): _description_ filters_conv3x3 (List[int]): _description_ filters_conv1x1 (List[int]): _description_ block_repetitions (List[int]): _description_ name (str): _description_ \"\"\" self . img_shape = img_shape self . filters_conv3x3 = filters_conv3x3 self . filters_conv1x1 = filters_conv1x1 self . block_repetitions = block_repetitions self . name = name self . endpoint_layers = [ \"maxpool_block1_out\" , \"maxpool_block2_out\" , \"maxpool_block3_out\" , \"maxpool_block4_out\" , ] def get_classification_backbone ( self ) -> Model : \"\"\"_summary_ Returns: Model: _description_ \"\"\" block1 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 0 ], filters_conv1x1 = self . filters_conv1x1 [ 0 ], name = f \"block_1_ { idx } \" , ) for idx in range ( self . block_repetitions [ 0 ]) ] block2 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 1 ], filters_conv1x1 = self . filters_conv1x1 [ 1 ], name = f \"block_2_ { idx } \" , ) for idx in range ( self . block_repetitions [ 1 ]) ] block3 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 2 ], filters_conv1x1 = self . filters_conv1x1 [ 2 ], name = f \"block_3_ { idx } \" , ) for idx in range ( self . block_repetitions [ 2 ]) ] block4 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 3 ], filters_conv1x1 = self . filters_conv1x1 [ 3 ], name = f \"block_4_ { idx } \" , ) for idx in range ( self . block_repetitions [ 3 ]) ] return Sequential ( [ Input ( self . img_shape ), ConvBNReLU ( filters = 64 , kernel_size = 3 , strides = 2 , name = \"stem_stage_1\" , ), ConvBNReLU ( filters = 64 , kernel_size = 3 , name = \"stem_stage_2\" , ), ConvBNReLU ( filters = 128 , kernel_size = 3 , name = \"stem_stage_3\" , ), * block1 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block1_out\" ), * block2 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block2_out\" ), * block3 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block3_out\" ), * block4 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block4_out\" ), ], name = self . name , ) def get_segmentation_backbone ( self ) -> Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Returns: A `tf.keras` model. \"\"\" backbone = self . get_classification_backbone () os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in self . endpoint_layers ] return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = self . name , )","title":"VoVNet"},{"location":"models/vovnet/#core_vision.models.vovnet.VoVNet.__init__","text":"summary Parameters: Name Type Description Default img_shape List [ int ] description required filters_conv3x3 List [ int ] description required filters_conv1x1 List [ int ] description required block_repetitions List [ int ] description required name str description required Source code in core_vision/models/vovnet.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self , img_shape : List [ int ], filters_conv3x3 : List [ int ], filters_conv1x1 : List [ int ], block_repetitions : List [ int ], name : str , ) -> None : \"\"\"_summary_ Args: img_shape (List[int]): _description_ filters_conv3x3 (List[int]): _description_ filters_conv1x1 (List[int]): _description_ block_repetitions (List[int]): _description_ name (str): _description_ \"\"\" self . img_shape = img_shape self . filters_conv3x3 = filters_conv3x3 self . filters_conv1x1 = filters_conv1x1 self . block_repetitions = block_repetitions self . name = name self . endpoint_layers = [ \"maxpool_block1_out\" , \"maxpool_block2_out\" , \"maxpool_block3_out\" , \"maxpool_block4_out\" , ]","title":"__init__()"},{"location":"models/vovnet/#core_vision.models.vovnet.VoVNet.get_classification_backbone","text":"summary Returns: Name Type Description Model Model description Source code in core_vision/models/vovnet.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def get_classification_backbone ( self ) -> Model : \"\"\"_summary_ Returns: Model: _description_ \"\"\" block1 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 0 ], filters_conv1x1 = self . filters_conv1x1 [ 0 ], name = f \"block_1_ { idx } \" , ) for idx in range ( self . block_repetitions [ 0 ]) ] block2 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 1 ], filters_conv1x1 = self . filters_conv1x1 [ 1 ], name = f \"block_2_ { idx } \" , ) for idx in range ( self . block_repetitions [ 1 ]) ] block3 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 2 ], filters_conv1x1 = self . filters_conv1x1 [ 2 ], name = f \"block_3_ { idx } \" , ) for idx in range ( self . block_repetitions [ 2 ]) ] block4 = [ OSAModule ( filters_conv3x3 = self . filters_conv3x3 [ 3 ], filters_conv1x1 = self . filters_conv1x1 [ 3 ], name = f \"block_4_ { idx } \" , ) for idx in range ( self . block_repetitions [ 3 ]) ] return Sequential ( [ Input ( self . img_shape ), ConvBNReLU ( filters = 64 , kernel_size = 3 , strides = 2 , name = \"stem_stage_1\" , ), ConvBNReLU ( filters = 64 , kernel_size = 3 , name = \"stem_stage_2\" , ), ConvBNReLU ( filters = 128 , kernel_size = 3 , name = \"stem_stage_3\" , ), * block1 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block1_out\" ), * block2 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block2_out\" ), * block3 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block3_out\" ), * block4 , MaxPool2D ( pool_size = ( 2 , 2 ), name = \"maxpool_block4_out\" ), ], name = self . name , )","title":"get_classification_backbone()"},{"location":"models/vovnet/#core_vision.models.vovnet.VoVNet.get_segmentation_backbone","text":"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Returns: Type Description Model A tf.keras model. Source code in core_vision/models/vovnet.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def get_segmentation_backbone ( self ) -> Model : \"\"\"Instantiate the model and use it as a backbone (feature extractor) for a semantic segmentation task. Returns: A `tf.keras` model. \"\"\" backbone = self . get_classification_backbone () os4_output , os8_output , os16_output , os32_output = [ backbone . get_layer ( layer_name ) . output for layer_name in self . endpoint_layers ] return Model ( inputs = [ backbone . input ], outputs = [ os4_output , os8_output , os16_output , os32_output ], name = self . name , )","title":"get_segmentation_backbone()"}]}